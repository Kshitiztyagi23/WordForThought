{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"provenance":[],"authorship_tag":"ABX9TyOvAL0K0D6UCYGamxLg4SLj"},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","execution_count":null,"metadata":{"id":"8eefJh0oKN44"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import os\n","import seaborn as sns\n","import matplotlib.pyplot as plt\n","import librosa\n","import librosa.display\n","from IPython.display import Audio\n","from sklearn.metrics import classification_report, accuracy_score\n","from sklearn.preprocessing import OneHotEncoder\n","from keras.models import Sequential\n","from keras.layers import Dense, LSTM, Dropout\n","\n","audio_file_paths = []\n","emotion_labels = []\n","\n","for directory_name, _, file_list in os.walk('/kaggle/input'):\n","    for file_name in file_list:\n","        audio_file_paths.append(os.path.join(directory_name, file_name))\n","        emotion = file_name.split('_')[-1].split('.')[0]\n","        emotion_labels.append(emotion.lower())\n","    if len(audio_file_paths) == 2800:\n","        break\n","print(f\"Number of audio files: {len(audio_file_paths)}\")\n","\n","audio_data = pd.DataFrame({'audio_path': audio_file_paths, 'emotion': emotion_labels})\n","print(audio_data.head())\n","print(audio_data['emotion'].value_counts())\n","\n","sns.countplot(data=audio_data, x='emotion')\n","plt.show()\n","\n","def plot_waveform(audio, sample_rate, emotion_name):\n","    plt.figure(figsize=(10, 4))\n","    plt.title(emotion_name, size=20)\n","    librosa.display.waveshow(audio, sr=sample_rate)\n","    plt.show()\n","\n","def plot_spectrogram(audio, sample_rate, emotion_name):\n","    stft_transformed = librosa.stft(audio)\n","    amplitude_db = librosa.amplitude_to_db(abs(stft_transformed))\n","    plt.figure(figsize=(11, 4))\n","    plt.title(emotion_name, size=20)\n","    librosa.display.specshow(amplitude_db, sr=sample_rate, x_axis='time', y_axis='hz')\n","    plt.colorbar()\n","    plt.show()\n","\n","emotions_to_visualize = ['fear', 'angry', 'disgust', 'neutral', 'sad', 'happy']\n","for emotion in emotions_to_visualize:\n","    sample_path = np.array(audio_data['audio_path'][audio_data['emotion'] == emotion])[0]\n","    sample_audio, sample_rate = librosa.load(sample_path)\n","    plot_waveform(sample_audio, sample_rate, emotion)\n","    plot_spectrogram(sample_audio, sample_rate, emotion)\n","    Audio(sample_path)\n","\n","def extract_mfcc_features(file_path):\n","    audio, sample_rate = librosa.load(file_path, duration=3, offset=0.5)\n","    mfcc_features = np.mean(librosa.feature.mfcc(y=audio, sr=sample_rate, n_mfcc=40).T, axis=0)\n","    return mfcc_features\n","\n","mfcc_features = audio_data['audio_path'].apply(lambda path: extract_mfcc_features(path))\n","mfcc_features_array = np.array([features for features in mfcc_features])\n","mfcc_features_array = np.expand_dims(mfcc_features_array, -1)\n","print(f\"Input features shape: {mfcc_features_array.shape}\")\n","\n","encoder = OneHotEncoder()\n","emotion_labels_encoded = encoder.fit_transform(audio_data[['emotion']]).toarray()\n","print(f\"Encoded labels shape: {emotion_labels_encoded.shape}\")\n","\n","from sklearn.model_selection import train_test_split\n","X_train, X_val, y_train, y_val = train_test_split(mfcc_features_array, emotion_labels_encoded, test_size=0.2, random_state=42)\n","\n","lstm_model = Sequential([\n","    LSTM(256, return_sequences=False, input_shape=(40, 1)),\n","    Dropout(0.2),\n","    Dense(128, activation='relu'),\n","    Dropout(0.2),\n","    Dense(64, activation='relu'),\n","    Dropout(0.2),\n","    Dense(7, activation='softmax')\n","])\n","\n","lstm_model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n","lstm_model.summary()\n","\n","history = lstm_model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=50, batch_size=64)\n","\n","epochs_range = range(50)\n","train_accuracy = history.history['accuracy']\n","validation_accuracy = history.history['val_accuracy']\n","plt.plot(epochs_range, train_accuracy, label='Training Accuracy')\n","plt.plot(epochs_range, validation_accuracy, label='Validation Accuracy')\n","plt.xlabel('Epochs')\n","plt.ylabel('Accuracy')\n","plt.legend()\n","plt.show()\n","\n","train_loss = history.history['loss']\n","validation_loss = history.history['val_loss']\n","plt.plot(epochs_range, train_loss, label='Training Loss')\n","plt.plot(epochs_range, validation_loss, label='Validation Loss')\n","plt.xlabel('Epochs')\n","plt.ylabel('Loss')\n","plt.legend()\n","plt.show()\n","\n","y_pred = lstm_model.predict(X_val)\n","y_pred_classes = np.argmax(y_pred, axis=1)\n","y_true_classes = np.argmax(y_val, axis=1)\n","\n","final_accuracy = accuracy_score(y_true_classes, y_pred_classes)\n","print(f\"Final Validation Accuracy: {final_accuracy * 100:.2f}%\")\n","\n","class_report = classification_report(y_true_classes, y_pred_classes, target_names=encoder.categories_[0])\n","print(\"\\nClassification Report:\\n\", class_report)\n"]}]}